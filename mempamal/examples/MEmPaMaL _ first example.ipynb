{
 "metadata": {
  "name": "MEmPaMaL : first example"
 }, 
 "name": "MEmPaMaL : first example", 
 "nbformat": 2, 
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown", 
     "source": "Introduction to MEmPaMaL\n========================\n\nExample with Scikit-learn\n-------------------------\n\nIn this example, we take the classical iris dataset."
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": "from mempamal.datasets import iris\nX, y = iris.get_data()", 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 1
    }, 
    {
     "cell_type": "markdown", 
     "source": "The pipeline will contains:\n\n- scaling of the data: centering and scaling wrt. the standard deviation\n- logistic regression with default parameters\n\nThe goodness of fit is estimated with:\n\n- a 5-folds (stratified) cross-validation\n- the score function is a F1 score\n"
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": "from sklearn.linear_model.logistic import LogisticRegression\nfrom sklearn.preprocessing.data import StandardScaler\nfrom sklearn.cross_validation import StratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import f1_score", 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 2
    }, 
    {
     "cell_type": "code", 
     "collapsed": true, 
     "input": "s1 = StandardScaler(with_mean=True, with_std=True)\ns2 = LogisticRegression()\np = [(\"scaler\", s1), (\"logit\", s2)]\nest = Pipeline(p)", 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 3
    }, 
    {
     "cell_type": "markdown", 
     "source": "Here is an illustration on only one of the folds:"
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": "fold_iter = StratifiedKFold(y, n_folds=5)\ntrain, test = fold_iter.__iter__().next()\nX_train, X_test = X[train], X[test]\ny_train, y_test = y[train], y[test]\ny_pred = est.fit(X_train, y_train).predict(X_test)\nf1_score(y_test, y_pred)", 
     "language": "python", 
     "outputs": [
      {
       "output_type": "pyout", 
       "prompt_number": 4, 
       "text": "0.89974937343358408"
      }
     ], 
     "prompt_number": 4
    }, 
    {
     "cell_type": "markdown", 
     "source": "Example with Scikit-learn + MEmPaMaL + Soma-Workflow\n----------------------------------------------------"
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": "from mempamal.configuration import JSONify_estimator, JSONify_cv, build_dataset\nfrom mempamal.workflow import create_wf, save_wf", 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 5
    }, 
    {
     "cell_type": "markdown", 
     "source": "We just take the same estimator:"
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": "s1 = StandardScaler(with_mean=True, with_std=True)\ns2 = LogisticRegression()\np = [(\"scaler\", s1), (\"logit\", s2)]\nest = Pipeline(p)", 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 6
    }, 
    {
     "cell_type": "markdown", 
     "source": "We jsonify the estimator and the cross-validation configuration:"
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": "method_conf = JSONify_estimator(est, \"logit__C\", out=\"./est.json\")\ncv_conf = JSONify_cv(StratifiedKFold, cv_kwargs={\"n_folds\": 5},\n                     score_func=f1_score, stratified=True,\n                     out=\"./cv.json\")", 
     "language": "python", 
     "outputs": [], 
     "prompt_number": 7
    }, 
    {
     "cell_type": "markdown", 
     "source": "We build the dataset in the current directory. \nIt's create a ``dataset.joblib`` file. \nThen we create the workflow in our internal format (``create_wf``). \nWith ``verbose=True``, it prints the commands on ``stdout``.\nAnd finally, we output the workflow (``save_wf``) in the soma-workflow format \nand write it to ``workflow.json`` (need soma-workflow)."
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": "dataset = build_dataset(X, y, method_conf, cv_conf, \".\")\nwfi = create_wf(dataset['folds'], cv_conf, method_conf, \".\",\n               verbose=True)\nwf = save_wf(wfi, \"./workflow.json\", mode=\"soma-workflow\")", 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": "python mempamal/scripts/mapper.py ./cv.json ./est.json ./dataset.joblib ./red_res_0.pkl 0\npython mempamal/scripts/mapper.py ./cv.json ./est.json ./dataset.joblib ./red_res_1.pkl 1\npython mempamal/scripts/mapper.py ./cv.json ./est.json ./dataset.joblib ./red_res_2.pkl 2\npython mempamal/scripts/mapper.py ./cv.json ./est.json ./dataset.joblib ./red_res_3.pkl 3\npython mempamal/scripts/mapper.py ./cv.json ./est.json ./dataset.joblib ./red_res_4.pkl 4\npython mempamal/scripts/outer_reducer.py ./final_res.pkl ./red_res_{outer}.pkl"
      }
     ], 
     "prompt_number": 8
    }, 
    {
     "cell_type": "markdown", 
     "source": "We print all the dependencies and we can check that \nthe *Final Reduce* depends on all *map tasks*."
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": "for dep in wfi[1]: print(dep)", 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": "('|--- Map outer=0', '|- Final reduce')\n('|--- Map outer=1', '|- Final reduce')\n('|--- Map outer=2', '|- Final reduce')\n('|--- Map outer=3', '|- Final reduce')\n('|--- Map outer=4', '|- Final reduce')"
      }
     ], 
     "prompt_number": 9
    }, 
    {
     "cell_type": "markdown", 
     "source": "Now, we create a ``WorkflowControler`` and we submit the workflow. \nWe wait for workflow completion then we read the final results."
    }, 
    {
     "cell_type": "code", 
     "collapsed": false, 
     "input": "from soma_workflow.client import WorkflowController\n\nimport time\nimport json\nimport sklearn.externals.joblib as joblib\n\ncontroler = WorkflowController()\nwf_id = controler.submit_workflow(workflow=wf, name=\"first example\")\n\nwhile controler.workflow_status(wf_id) != 'workflow_done':\n    time.sleep(2)\nprint(joblib.load('./final_res.pkl'))", 
     "language": "python", 
     "outputs": [
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": "light mode\n{'std': 0.013628456581812496, 'raw': array([ 0.89769821,  0.93324979,  0.89974937,  0.89974937,  0.89974937]), 'median': 0.89974937343358408, 'mean': 0.90603922423279004}"
      }, 
      {
       "output_type": "stream", 
       "stream": "stdout", 
       "text": ""
      }
     ], 
     "prompt_number": 10
    }
   ]
  }
 ]
}